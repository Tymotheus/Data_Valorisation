---
title: "How do education and social features affect STEM salaries? "
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Introduction

This notebook is the second delivery for Valorization des doneés course, in Université Cote de Azur, Nice (2023) . You can watch the explanation for the aim of the project and the data cleaning [here](https://www.youtube.com/watch?v=QIKk9Wc0Io89).

-   Business goal: Analyze how social features( race, gender)and education affect STEM salaries.

-   Technical goal: Perform regression to predict total yearly income

## Authors

Tymoteusz Igor Ciesielski [*tymoteusz-igor.ciesielski\@etu.univ-cotedazur.fr*](mailto:tymoteusz-igor.ciesielski@etu.univ-cotedazur.fr){.email}

Lucia Trillo Carreras [*lucia.trillo-carreras\@etu.univ-cotedazur.fr*](mailto:lucia.trillo-carreras@etu.univ-cotedazur.fr){.email}

Ewa Kupczyk [*ewa.kupczyk\@etu.univ-cotedazur.fr*](mailto:ewa.kupczyk@etu.univ-cotedazur.fr){.email}

Marina Bueno Garcia [*marina.bueno-garcia\@etu.univ-cotedazur.fr*](mailto:marina.bueno-garcia@etu.univ-cotedazur.fr){.email}

## Libraries

```{r}
library(tidyverse)
library(ggplot2)
library(plotly) #interactive visualisation
library(lattice) #data exploration (corellations)
library(dplyr)
library(countrycode) # For adding continent variable
library(corrplot)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caTools)
library(MLmetrics)
library(e1071)
library(plot3D)
```

## Data

The salary dataset we have used is scraped from Levels Fyi, a US-based online service for compensation comparison across companies in Tech. The dataset contains 62 642 samples and 29 variables.  Taking into consideration the type of data we can distinguish:

-   12 categorical nominal variables

-   6 categorical ordinal variables

-   2 numerical discrete variables and  4 numerical continuous variables

To check the detailed preprocessing, see \*notebookfirstdelivery.Rmd\* in the same repo.

We import our cleaned dataset from the first delivery

```{r}

#Dataset with all the variables
#df_onehot <- read.csv('/Users/ltcmu/OneDrive/Documentos/Master/Data Valorisation/Proyecto/clean_salary_data.csv')
df_onehot <- read.csv('C:/Users/Tymoteusz/Desktop/Data_Valorisation/clean_salary_data.csv')

df_onehot <- na.omit(df_onehot) #Ensure we dont have NA values

#Dataset with Race, Gender and Education
df_social<-df_onehot[c("totalyearlycompensation","ed_master","ed_doctor","race_asian","race_hispanic","race_black","race_two_or_more","gender_female","gender_other")]
df_social <- na.omit(df_social) #Ensure we dont have NA values
```

We will feed our regression model with the one-hot encoded variables, having previously eliminating those with high correlation.

```{r}
df_onehot
dim(df_onehot)
```

We define our output variable Y:

```{r}
Y <- df_onehot[c("totalyearlycompensation")]
df_onehot <- subset(df_onehot, select = -c(totalyearlycompensation))
```

We split the data into training and testing sets

```{r}
set.seed(123)
trainIndex <- sample(1:nrow(df_onehot), 0.7 * nrow(df_onehot))
Ytrain <- Y[trainIndex,]
trainData <- df_onehot[trainIndex,]
Ytest <- Y[-trainIndex,]
testData <- df_onehot[-trainIndex,]
```

## LINEAR REGRESSION

Fit the linear regression model using the training and predict using the testing data.

```{r}
linearModel <- lm(Ytrain ~ ., data = trainData)
predictions <- predict(linearModel, newdata = testData)
```

Evaluate our model plotting the residuals and calculating the MSE:

```{r}
residuals <- Ytest - predictions
plot(residuals ~ predictions, xlab = "Predictions", ylab = "Residuals",
     main = "Residuals vs. Predictions")
abline(h = 0, col = "red")

mse <- mean(residuals^2)
cat("Mean Squared Error:", mse, "\n")

summary(Y)
```

Finally, we analyze the model:

```{r}
summary(linearModel)
```

## SVR

SVR is a type of supervised machine learning algorithm used for classification and regression analysis.

We implement 6 different models:

-   Linear kernel + nu-regression

```{r}
svmModel_nu <- svm(Ytrain ~ ., data = trainData, kernel = "linear", type = "nu-regression")

summary(svmModel_nu)
coef(svmModel_nu)

# Test your SVM model on the testing set
svmPred_nu <- predict(svmModel_nu, testData)

# Evaluate the accuracy of your SVM model: RMSE
RMSE_nu <- sqrt(mean((svmPred_nu - Ytest)^2))
RMSE_nu
```

-   Linear kernel + eps-regression

```{r}
svmModel_eps <- svm(Ytrain ~ ., data = trainData, kernel = "linear", type="eps-regression")
summary(svmModel_eps)
coef(svmModel_eps)
svmPred_eps <- predict(svmModel_eps, testData)
RMSE_eps <- sqrt(mean((svmPred_eps - Ytest)^2))
RMSE_eps
```

-   Radial kernel + nu-regression

```{r}
svmModel_nu_ra <- svm(Ytrain ~ ., data = trainData, kernel = "radial",type = "nu-regression")
summary(svmModel_nu_ra)
svmPred_nu_ra <- predict(svmModel_nu_ra, testData)
RMSE_nu_ra <- sqrt(mean((svmPred_nu_ra - Ytest)^2))
RMSE_nu_ra
```

-   Radial kernel + eps-regression

```{r}
svmModel_eps_ra <- svm(Ytrain ~ ., data = trainData, kernel = "radial", type = "eps-regression")
summary(svmModel_eps_ra)
svmPred_eps_ra <- predict(svmModel_eps_ra, testData)
RMSE_eps_ra <- sqrt(mean((svmPred_eps_ra - Ytest)^2))
RMSE_eps_ra
```

-   Polynomial kernel + nu-regression

```{r}
svmModel_nu_po <- svm(Ytrain ~ ., data = trainData, kernel = "polynomial", type = "nu-regression")
summary(svmModel_nu_po)
svmPred_nu_po <- predict(svmModel_nu_po, testData)
RMSE_nu_po <- sqrt(mean((svmPred_nu_po - Ytest)^2))
RMSE_nu_po
```

-   Polynomial kernel + eps-regression

```{r}
svmModel_eps_po <- svm(Ytrain ~ ., data = trainData, kernel = "polynomial", type = "eps-regression")
summary(svmModel_eps_po)
svmPred_eps_po <- predict(svmModel_eps_po, testData)
RMSE_eps_po <- sqrt(mean((svmPred_eps_po - Ytest)^2))
RMSE_eps_po
```

The model with the lowest RMSE is svmModel_nu_ra (radial kernel + nu-regression), followed closely by svmModel_nu (linear kernel + nu-regression).

The residual plot of this model is:

```{r}
residuals <- Ytest - svmPred_nu_ra
plot(svmPred_nu_ra, residuals, main="Residual Plot", xlab="Predicted Values", ylab="Residuals")
plot(Ytest,residuals, main="Residual Plot (radial + nu regression)", xlab="Test Values", ylab="Residuals")
abline(h = 0, col = "red")
```

And the Predicted vs Actual Values graph is:

```{r}
lims <- range(c(Ytest, svmPred_nu_ra))
plot(Ytest, svmPred_nu_ra, main = "Predicted vs Actual Values (radial + nu regression)", xlab = "Actual Values", ylab = "Predicted Values", xlim = lims, ylim = lims)
abline(0, 1, col = "red")
```

To have an idea of how good/bad our results are, let's have a look at the summary of the output variable

```{r}
summary(Ytest)
```

We calculate the R-squared value of our best model:

```{r}
R2_nu_ra <- 1 - sum((Ytest - svmPred_nu_ra)^2) / sum((Ytest - mean(Ytest))^2)
R2_nu_ra
```

This graph compares the real salary values and predictions for the four models:

```{r}
# Create a data frame with the scaled salaries and predictions
dfgraph <- data.frame(scaled_salaries = Ytest, pred_nu =svmPred_nu, pred_eps =svmPred_eps,pred_nu_ra=svmPred_nu_ra,pred_eps_ra=svmPred_eps_ra)

# Sort the data frame by the scaled salaries for plotting
dfgraph <- dfgraph[order(dfgraph$scaled_salaries), ]

# Create a line plot of the scaled salaries and predictions

plot_ly(dfgraph, x = ~scaled_salaries) %>%
 add_lines(y = ~pred_eps, name = "Linear kernel + nu-regression", line = list(color = "red", width = 2)) %>%
  add_lines(y = ~pred_nu_ra, name = "Linear kernel + eps-regression", line = list(color = "green", width = 2)) %>%
  add_lines(y = ~pred_eps_ra, name = "Radial kernel + nu-regression", line = list(color = "blue", width = 2)) %>%
  add_lines(y = ~pred_eps_ra, name = "Radial kernel + eps-regression", line = list(color = "yellow", width = 2)) %>%
  layout(xaxis = list(title = "Real scaled Salaries"), yaxis = list(title = "Predicted scaled Salaries"), ylim = c(0, 6))


```

## Regression Decision Trees and Random Forests

\
We continued analysis using the regression trees and random forest methods

```{r}
## Regression tree
df_onehot <- read.csv('C:/Users/Tymoteusz/Desktop/Data_Valorisation/full_one_hot_data.csv')
df_onehot <- na.omit(df_onehot)

# Whole one-hot encoded data frame, all the parameters
full_tree <- rpart(totalyearlycompensation~., data=df_onehot, method='anova')
rpart.plot(full_tree)
```

```{r}
# Building a tree only based on the social features: race, gender, education
social_tree <- rpart(totalyearlycompensation~ed_master+ed_bachelor+ed_doctor+
                  race_asian+race_hispanic+race_white+ race_black+ race_two_or_more+
                  gender_female+ gender_other,
                  data=df_onehot, method='anova', control = rpart.control(cp = 0.005))
rpart.plot(social_tree)
```

```{r}
# Removing education from the social features
non_ed_tree <- rpart(totalyearlycompensation~race_asian+ race_hispanic+race_white+ race_black+ race_two_or_more+
                      gender_female+ gender_other,
                     data=df_onehot, method='anova', control = rpart.control(cp = 0.0005))
rpart.plot(non_ed_tree)
```

```{r}
# As an additional analysis we applied classification tree on non-one-hot-encoded data 
classification_tree <- rpart(Education~gender+
                               Race_Asian+Race_White+Race_Two_Or_More+Race_Black+Race_Hispanic,
              data=df, method='class')
rpart.plot(classification_tree)
```

```{r}
# Random forest compared with the regression tree
# We create train and test set, we train both our models and then validate their accuracy
split <- sample.split(df_onehot, SplitRatio=0.8)
train <- subset(df_onehot, split == "TRUE")
test <- subset(df_onehot, split == "FALSE")
set.seed(123) # We set it to obtain always the same results, in production environment we would skip this step
classifier_RF = randomForest(x = train, y= train$totalyearlycompensation, ntree=50, keep.forest=TRUE)
y_pred_RF = predict(newdata=test, object=classifier_RF)
classifier_RT <- rpart(totalyearlycompensation~., data=test, method="anova" )
y_pred_RT = predict(newdata=test, object=classifier_RT)

# Comparing the chosen accuracy metric (RMSE) between regression tree and random forest 
rmse_RF = sqrt(MSE(y_pred_RF, test$totalyearlycompensation)) # ~10k
rmse_RT = sqrt(MSE(y_pred_RT, test$totalyearlycompensation)) # ~100k
```

```{r}
sprintf("RMSE for the regression tree method: %s", rmse_RT)
sprintf("RMSE for the random forest method: %s", rmse_RF)
```

```{r}
## Alternative methods for finding the cp parameter - the optimal one - for building the trees
## Calculated according to the lab 8
########
## Alternative full one-hot data
alt_full_tree <- rpart( totalyearlycompensation~., 
                     data = df_onehot, 
                     method='anova', # output is a categorical variable by asking for method=’class’
                     control = rpart.control(
                       xval = 10, # number of cross-validations
                       minbucket = 2, # the minimum number of observations in any terminal (leaf) node
                       cp = 0.0)
)
optimalCp = alt_full_tree$cptable[which.min(alt_full_tree$cptable[,4]),1]
alt_full_tree_Optimal <- prune(alt_full_tree, cp=optimalCp)
#Plot the decision tree
rpart.plot(alt_full_tree_Optimal,type=3)
```

```{r}
## Alternative data with all social features
alt_social_tree <- rpart(totalyearlycompensation~ed_master+ed_other+ed_doctor+
                           race_asian+ race_hispanic+ race_black+ race_two_or_more+
                           gender_female+ gender_other,
                        data = df_onehot, 
                        method='anova', # output is a categorical variable by asking for method=’class’
                        control = rpart.control(
                          xval = 10, # number of cross-validations
                          minbucket = 2, # the minimum number of observations in any terminal (leaf) node
                          cp = 0.0)
)
optimalCp = alt_social_tree$cptable[which.min(alt_social_tree$cptable[,4]),1]
alt_social_tree_Optimal <- prune(alt_social_tree, cp=optimalCp)
#Plot the decision tree
rpart.plot(alt_social_tree_Optimal,type=3)
```

```{r}
## Alternative data with all social features except education
alt_non_ed_tree <- rpart(totalyearlycompensation~race_asian+ race_hispanic+ race_black+ race_two_or_more+
                           gender_female+ gender_other,
                         data = df_onehot, 
                         method='anova', # output is a categorical variable by asking for method=’class’
                         control = rpart.control(
                           xval = 10, # number of cross-validations
                           minbucket = 2, # the minimum number of observations in any terminal (leaf) node
                           cp = 0.0)
)
optimalCp = alt_non_ed_tree$cptable[which.min(alt_non_ed_tree$cptable[,4]),1]
alt_non_ed_tree_Optimal <- prune(alt_non_ed_tree, cp=optimalCp)
#Plot the decision tree
rpart.plot(alt_non_ed_tree_Optimal,type=3)
##########
```

\
