---
title: "Second delivery Kaggle Challenge"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Team: "Ewa, Tymo, Luc√≠a, Marina"

## LINEAR REGRESSION

We feed our regression model with the one-hot encoded variables, eliminating those with high correlation.

```{r}
df_social<-df_onehot[c("totalyearlycompensation","ed_other","ed_master","ed_doctor","race_asian","race_hispanic","race_black","race_two_or_more","gender_female","gender_other")]

df_social <- na.omit(df_onehot)

df_social <- subset(df_social, select = -c(year_2018, year_2019, year_2021,com_ibm,title_hweng,continent_europe))
```

We define our output variable:

```{r}
Y <- df_social[c("totalyearlycompensation")]
df_social <- subset(df_social, select = -c(totalyearlycompensation))
```

We split the data into training and testing sets

```{r}
set.seed(123)
trainIndex <- sample(1:nrow(df_social), 0.7 * nrow(df_social))
Ytrain <- Y[trainIndex,]
trainData <- df_social[trainIndex,]
Ytest <- Y[-trainIndex,]
testData <- df_social[-trainIndex,]
```

Fit the linear regression model using the training and predict using the testing data.

```{r}
linearModel <- lm(Ytrain ~ ., data = trainData)
predictions <- predict(linearModel, newdata = testData)
```

Evaluate our model plotting the residuals and calculating the MSE:

```{r}
residuals <- Ytest - predictions
plot(residuals ~ predictions, xlab = "Predictions", ylab = "Residuals",
     main = "Residuals vs. Predictions")
abline(h = 0, col = "red")

mse <- mean(residuals^2)
cat("Mean Squared Error:", mse, "\n")

summary(Y)
```

Finally, we analyze the model:

```{r}
summary(linearModel)
```

## SVR

SVR is a type of supervised machine learning algorithm used for classification and regression analysis.

We implement 6 different models:

-   Linear kernel + nu-regression

```{r}
svmModel_nu <- svm(Ytrain ~ ., data = trainData, kernel = "linear", type = "nu-regression")

summary(svmModel_nu)
coef(svmModel_nu)

# Test your SVM model on the testing set
svmPred_nu <- predict(svmModel_nu, testData)

# Evaluate the accuracy of your SVM model: RMSE
RMSE_nu <- sqrt(mean((svmPred_nu - Ytest)^2))
RMSE_nu
```

-   Linear kernel + eps-regression

```{r}
svmModel_eps <- svm(Ytrain ~ ., data = trainData, kernel = "linear", type="eps-regression")
summary(svmModel_eps)
coef(svmModel_eps)
svmPred_eps <- predict(svmModel_eps, testData)
RMSE_eps <- sqrt(mean((svmPred_eps - Ytest)^2))
RMSE_eps
```

-   Radial kernel + nu-regression

```{r}
svmModel_nu_ra <- svm(Ytrain ~ ., data = trainData, kernel = "radial",type = "nu-regression")
summary(svmModel_nu_ra)
svmPred_nu_ra <- predict(svmModel_nu_ra, testData)
RMSE_nu_ra <- sqrt(mean((svmPred_nu_ra - Ytest)^2))
RMSE_nu_ra
```

-   Radial kernel + eps-regression

```{r}
svmModel_eps_ra <- svm(Ytrain ~ ., data = trainData, kernel = "radial", type = "eps-regression")
summary(svmModel_eps_ra)
svmPred_eps_ra <- predict(svmModel_eps_ra, testData)
RMSE_eps_ra <- sqrt(mean((svmPred_eps_ra - Ytest)^2))
RMSE_eps_ra
```

-   Polynomial kernel + nu-regression

```{r}
svmModel_nu_po <- svm(Ytrain ~ ., data = trainData, kernel = "polynomial", type = "nu-regression")
summary(svmModel_nu_po)
svmPred_nu_po <- predict(svmModel_nu_po, testData)
RMSE_nu_po <- sqrt(mean((svmPred_nu_po - Ytest)^2))
RMSE_nu_po
```

-   Polynomial kernel + eps-regression

```{r}
svmModel_eps_po <- svm(Ytrain ~ ., data = trainData, kernel = "polynomial", type = "eps-regression")
summary(svmModel_eps_po)
svmPred_eps_po <- predict(svmModel_eps_po, testData)
RMSE_eps_po <- sqrt(mean((svmPred_eps_po - Ytest)^2))
RMSE_eps_po
```

The model with the lowest RMSE is svmModel_nu_ra (radial kernel + nu-regression), followed closely by svmModel_nu (linear kernel + nu-regression).

The residual plot of this model is:

```{r}
residuals <- Ytest - svmPred_nu_ra
plot(svmPred_nu_ra, residuals, main="Residual Plot", xlab="Predicted Values", ylab="Residuals")
plot(Ytest,residuals, main="Residual Plot (radial + nu regression)", xlab="Test Values", ylab="Residuals")
abline(h = 0, col = "red")
```

And the Predicted vs Actual Values graph is:

```{r}
lims <- range(c(Ytest, svmPred_nu_ra))
plot(Ytest, svmPred_nu_ra, main = "Predicted vs Actual Values (radial + nu regression)", xlab = "Actual Values", ylab = "Predicted Values", xlim = lims, ylim = lims)
abline(0, 1, col = "red")
```

To have an idea of how good/bad our results are, let's have a look at the summary of the output variable

```{r}
summary(Ytest)
```

We calculate the R-squared value of our best model:

```{r}
R2_nu_ra <- 1 - sum((Ytest - svmPred_nu_ra)^2) / sum((Ytest - mean(Ytest))^2)
R2_nu_ra
```
